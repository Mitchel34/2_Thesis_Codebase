%% To submit your paper:
\documentclass[draft]{agujournal2019}
\usepackage{url}
\usepackage{lineno}
\usepackage[inline]{trackchanges}
\usepackage{soul}
\linenumbers

\draftfalse

\journalname{Water Resources Research}

\begin{document}

\title{Site-Aware Residual Corrections to National Water Model Streamflow Using Hybrid Transformers and Recurrent Baselines}

\authors{[Mitchel Carson]\affil{1}, [Author Two]\affil{1,2}, and [Author Three]\affil{2}}

\affiliation{1}{[Computer Science], [Appalachian State University], [Boone], [North Carolina], [United States of America]}
\affiliation{2}{[Institute or Lab], [City], [State], [Country]}

\correspondingauthor{[Corresponding Author Name]}{[email@address.edu]}

\begin{keypoints}
\item We build a leakage-safe, hourly dataset (USGS, NWM, ERA5, NLCD) spanning 2010--2022 to support residual post-processing of NWM streamflow.
\item A streamlined GRU--transformer residual model delivers up to a 20.6\% RMSE reduction; a lighter quantile-trimmed bias shift trades to a 13.4\% gain (NSE~0.64, KGE~0.72) while shrinking percent bias to \(-2.5\%\).
\item Reproducible pipelines, quantile calibration, and documented hyperparameter sweeps set the stage for future site-to-site scaling toward \(>25\%\) improvements.
\end{keypoints}

\begin{abstract}
Operational streamflow forecasts from the National Water Model (NWM) are widely used yet exhibit site- and regime-specific errors. We construct an hourly, site-aligned dataset (2010--2022) combining USGS discharge (ground truth), NWM CHRTOUT retrospective and operational analyses (baseline), ERA5 atmospheric reanalysis (forcings), and NLCD 2021 static metrics (physiography). Building on this dataset, we benchmark a minimalist GRU--transformer residual model that predicts \(y_{\text{USGS}} - y_{\text{NWM}}\) and reconstructs corrected flows as \(y_{\text{NWM}} + \hat{r}\), trained with heteroscedastic likelihoods, NSE surrogates, and median-focused quantile losses. Evaluated on the held-out 2022 window, the model reduces RMSE from 5.97 to 4.74~cms (20.6\% improvement) but exhibits \(-9.1\%\) bias; adding a quantile-trimmed residual shift yields a gentler 5.17~cms RMSE (13.4\% improvement) with percent bias improved to \(-2.5\%\) while maintaining NSE~0.64 and KGE~0.72. We document the leakage-safe pipeline, calibration diagnostics, and remaining bias challenges, outlining the next steps needed to approach \(>25\%\) improvement across diverse basins.
\end{abstract}

\section*{Plain Language Summary}
Forecasts from the U.S. National Water Model are valuable for rivers and streams but can be biased at specific locations. We built an hourly dataset that combines the model’s predictions, real measurements from USGS gauges, weather reanalysis, and local land-cover information. We then trained machine learning models to learn the difference between the model and reality (the “residual”) and add that back to correct the forecast. A transformer-based model provided the most consistent improvement in a winter flood period, while a simpler recurrent model did not perform as well. We outline how we aligned the data safely and what we will do next to make the corrections more reliable year-round.

\section{Introduction}
Physics-based hydrologic models such as the National Water Model (NWM) provide high-coverage streamflow guidance but retain site-specific and regime-dependent errors. Data-driven post-processing that corrects model residuals can deliver practical operational gains if it preserves temporal causality, avoids information leakage, and scales across sites and years. This study develops an hourly, multi-source dataset and evaluates modern sequence models for site-aware residual correction, emphasizing reproducibility and fair comparisons across architectures.

Our working hypothesis is that a rich suite of meteorological covariates—particularly those available from ERA5 and ERA5-Land—encodes much of the physics that drive systematic NWM errors. By explicitly supplying local precipitation, radiation, thermodynamic, and wind signals alongside land-cover descriptors, we expect a learning system to anticipate when NWM will under- or over-predict discharge and to correct those deviations in a causal manner. Although the present work focuses on one well-instrumented watershed, the broader goal is to build a template that can be retrained site-by-site and eventually scaled across diverse U.S. biomes. We therefore document each pipeline step with an eye toward portability so that, once the residual learner is mature, we can deploy it wherever ERA5 and basic physiographic data are available. To further reduce temporal overfitting we plan to evaluate models with rolling-origin time-series cross-validation, an approach shown to produce unbiased risk estimates for dependent data \\citep{bergmeir2012,hyndman2021}.

\section{Materials and Methods}
\subsection{End-to-End Pipeline Overview}
Our workflow converts disparate hydrometeorological feeds into reproducible, leakage-safe training and evaluation artefacts. The major stages are: (1) raw acquisition of USGS discharge, NWM CHRTOUT retrospective/op analysis, ERA5 reanalysis, and NLCD 2021 land-cover metrics; (2) harmonisation of spatial identifiers, units, and timestamps; (3) feature engineering and normalisation driven strictly by train-period statistics; (4) sequence dataset assembly with causal windows and deterministic splits; and (5) model training, validation, and transparent metric reporting. Throughout we prefer open, versioned sources and log every command to enable replay.

\paragraph{Diagram placeholder.} Future Figure~1 will present a swim-lane diagram illustrating how each data source flows through acquisition, preprocessing, and batching stages.

\subsection{Raw Data Acquisition}
\subsubsection{USGS Gauged Discharge}
NWIS CSV endpoints are queried per site with bounded concurrency, exponential backoff, and nightly retries. We resample to hourly cadence, convert cfs to cms, and drop points with flagged quality codes. Output CSVs under \texttt{data/raw/usgs/} serve as authoritative ground truth.

\subsubsection{NWM Baseline Forecasts}
Retrospective v3.0 CHRTOUT and operational analysis files are downloaded via \texttt{boto3}. Hourly streamflow is extracted by COMID, timestamps normalised to naive UTC, and duplicate \((\text{timestamp},\text{COMID})\) records removed. Retrospective and operational eras are stitched into a single time series per site.

\subsubsection{ERA5 Meteorological Forcings}
Monthly ERA5-Land subsets around each gauge are downloaded and interpolated to match gauge timestamps. Variables include temperature, dew point, pressure, precipitation, radiation, wind components, vapour pressure deficit, and derived trigonometric time encodings.

\subsubsection{NLCD Static Attributes}
NLCD 2021 fractions for urban, forest, agriculture, impervious, and wetlands classes are extracted from precomputed rasters. Regulation status flags are encoded to produce binary “is regulated” features.

\paragraph{Diagram placeholder.} Future Figure~2 will depict the geographic context—map of study sites alongside representative NLCD land-cover tiles.

\subsection{Preprocessing and Harmonisation}
\subsubsection{Temporal Alignment}
All sources are aligned to the USGS time axis. We enforce left-aligned, causal joins so the residual target at time \(t\) depends only on observations up to \(t\). Missing values are imputed only when explicitly justified (e.g., static features); otherwise rows are dropped and gaps documented.

\subsubsection{Feature Engineering}
Dynamic ERA5 features are standardised, trigonometric encodings (\(\sin/\cos\)) for hour-of-day, day-of-year, and month-of-year are added, and NLCD percentages are normalised to percentages summing to 100. Residual targets are defined as \(y_{\text{residual}} = y_{\text{USGS}} - y_{\text{NWM}}\); corrected targets serve for evaluation only to avoid label leakage.

\subsubsection{Normalisation Strategy}
Scaling statistics (mean, standard deviation) are computed over training-period rows only and cached. These statistics are reused verbatim for validation/test to avoid leakage. Static features are z-scored; dynamic inputs use the same per-feature statistics to guarantee comparability across sequences.

\paragraph{Diagram placeholder.} Future Figure~3 will show a table schematic with raw-to-standardised conversions and highlight guardrails against temporal leakage.

\subsection{Dataset Assembly and Splits}
\subsubsection{Sequence Construction}
We wrap the aligned dataframe in a \texttt{SeqDataset} class that emits rolling windows of length \(L\) with stride one. Each sample returns (i) dynamic sequence, (ii) static vector, (iii) residual target, (iv) observed discharge, and (v) baseline NWM.

\subsubsection{Splitting Protocol}
For current experiments we adopt 2010--2020 as training, 2021 as validation, and 2022 as the held-out test set. The loader enforces a single-site invariant to prevent cross-basin leakage and raises if multiple COMIDs appear.

\paragraph{Diagram placeholder.} Future Figure~4 will provide a timeline illustrating the rolling window mechanism, highlighting how training, validation, and test slices are constructed without overlap.

\subsection{Training Workflow}
\subsubsection{Optimisation Loop}
Mini-batches are drawn with causal order preserved. We employ AdamW (or Ranger when available) with gradient clipping, optional mixed precision, and cosine or plateau schedulers. Early stopping monitors validation loss with a patience of 4--6 epochs.

\subsubsection{Loss Composition}
The core objective combines: (i) heteroscedastic Gaussian negative log-likelihood on the residual; (ii) the same likelihood on the corrected flow derived as \(y_{\text{NWM}} + \hat{y}_{\text{residual}}\); (iii) an optional NSE surrogate; (iv) median-weighted quantile pinball loss; and (v) a mild squared mean-bias penalty. Flow-weighted factors emphasise high-magnitude events without destabilising low-flow performance.

\paragraph{Diagram placeholder.} Future Figure~5 will outline the training loop, including loss components and gradient flow from residual and corrected outputs.

\subsection{Evaluation and Reporting}
After training we reload the best checkpoint (lowest validation loss) and generate test-set predictions. Metrics include RMSE, MAE, NSE, Kling-Gupta efficiency, percent bias, Pearson, and Spearman correlations. We also compute quantile coverage/bias for each probabilistic head and archive predictions, metrics JSON, and configuration command strings under versioned directories to ensure reproducibility.

\paragraph{Diagram placeholder.} Future Figure~6 will visualise cumulative distribution improvements and quantile coverage, providing an at-a-glance comparison between NWM and corrected forecasts.

\section{Model Architecture}
\subsection{Transformer Primer}
Transformers model sequential data using stacked self-attention and position-wise feed-forward layers wrapped in residual connections. Each encoder block projects the sequence into query, key, and value representations, computes scaled dot-product attention, and refines the representation through a feed-forward sublayer. Layer normalisation and residual paths stabilise training, while positional encodings (sinusoidal or learned) preserve order information absent in the attention mechanism itself.

\paragraph{Diagram placeholder.} Future Figure~7 will sketch a generic transformer encoder, labelling key components (self-attention, feed-forward, residual/normalisation) alongside the positional encodings.

\subsection{Minimal Residual Transformer Baseline}
Our baseline couples a lightweight GRU pre-encoder with a two-layer transformer encoder (\(d_\text{model}=64\), four heads) to capture both sequential memory and multi-scale context. Static NLCD embeddings are projected once and concatenated with pooled transformer summaries (last token and mean pooling), then passed through a compact fusion MLP. The network predicts (i) the residual mean, (ii) residual log-variance, and (iii) corrected-flow log-variance. Corrected flows are recovered as \(y_{\text{NWM}} + \hat{y}_{\text{residual}}\), anchoring the model to the physical baseline. Optional quantile heads provide calibrated distributional forecasts for decision support.

\paragraph{Diagram placeholder.} Future Figure~8 will depict the baseline architecture: GRU front-end, transformer encoder blocks, fusion layer, and output heads, highlighting data paths for dynamic, static, and residual channels.

\subsection{Design Elements We Intend to Preserve}
As we iterate toward \(\geq 25\%\) RMSE improvement over raw NWM forecasts, several architectural choices have proven essential:
\begin{enumerate}
    \item \textbf{Residual-first formulation.} Predicting the residual and reconstructing corrected flow preserves physical consistency and simplifies calibration.
    \item \textbf{GRU $+$ Transformer fusion.} The GRU captures short-term persistence, while attention layers offer long-range context with modest parameter growth.
    \item \textbf{Heteroscedastic outputs.} Log-variance heads stabilise training and enable uncertainty-aware evaluation.
    \item \textbf{Median-focused quantile loss.} Weighting the 0.5 quantile encourages unbiased central tendencies while maintaining tail awareness.
    \item \textbf{Consistency constraints.} Penalising divergence between derived and direct corrected outputs keeps the model anchored to physically plausible corrections.
\end{enumerate}

\paragraph{Diagram placeholder.} Future Figure~9 will compare incremental architectural variants (baseline, additional cross-attention, multi-scale pooling) and annotate expected gains.

\section{Results to Date}
Our latest GRU-transformer residual model attains a 4.74~cms RMSE (20.6\% reduction) relative to the raw NWM baseline, although the corresponding percent bias remains \(-9.1\%\). Applying the new quantile-trimmed residual shift—averaging validation residuals between the 5th and 90th flow quantiles and tapering weights toward low flows—yields a 5.17~cms RMSE (13.4\% reduction) with percent bias reduced to \(-2.5\%\), NSE~0.64, and KGE~0.72. Median-focused quantile weighting continues to lift q50 coverage above 0.60, and further calibration work will target simultaneous improvements across RMSE, NSE, KGE, and PBIAS. All experiments are archived (commands plus metrics) to provide auditable baselines for future iterations.

\paragraph{Diagram placeholder.} Future Figure~10 will show side-by-side hydrographs for select sites, illustrating baseline vs corrected flow, residuals, and forecast uncertainty envelopes.

\section{Discussion and Future Work}
Upcoming work will (i) broaden the hyperparameter search over consistency and NSE weights now that the richer baseline is stable; (ii) explore cross-attention between dynamic and static channels; (iii) integrate bias-aware calibration objectives; and (iv) package the pipeline into reproducible releases. Achieving \(\geq 25\%\) improvement will likely require both architectural refinement and calibrated loss shaping, and the current documentation provides the scaffold for those iterations.

\subsection{Downstream Decision Support Opportunities}
The National Water Model feeds the National Weather Service’s National Water Prediction Service (NWPS), where hourly to multi-day discharge forecasts populate map-based products, AWIPS decision dashboards, and River Forecast Center workflows. Operational hydrologists blend NWM guidance with local hydraulic models and expert judgment to issue flood watches, reservoir operations advisories, and navigation bulletins; partner agencies such as USACE, USGS, and emergency management offices consume the same feeds for situational awareness and resource allocation. Because these services depend on rapid access to bias-aware streamflow predictions, a transformer-based post-processor can slot in as a lightweight corrective layer: corrected flows can be published alongside raw NWM guidance, enabling forecasters to see both the baseline and the bias-adjusted ensemble, or ingested into automated alert thresholds where reduced percent-bias and improved NSE/KGE translate directly into fewer false alarms and missed events. Beyond deterministic guidance, the transformer’s quantile head offers calibrated uncertainty summaries that could support probabilistic flood triggers once post-hoc calibration is complete. Integrating the post-processor therefore promises not only improved accuracy metrics but also more trustworthy decision support across flood warning, reservoir scheduling, and impact-based communication.

\begin{thebibliography}{99}
ibitem[\textit{Bergmeir and Ben\'{i}tez}(2012)]{bergmeir2012} Bergmeir, C., and J. M. Ben\'{i}tez (2012), On the use of cross-validation for time series predictor evaluation, \textit{Information Sciences}, \textit{191}, 192--213, doi:10.1016/j.ins.2011.12.028.

ibitem[\textit{Hyndman and Athanasopoulos}(2021)]{hyndman2021} Hyndman, R. J., and G. Athanasopoulos (2021), \textit{Forecasting: Principles and Practice}, 3rd ed., OTexts, Melbourne, Australia. Retrieved from https://otexts.com/fpp3/.
\end{thebibliography}

\end{document}
