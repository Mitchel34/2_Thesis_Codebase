%% To submit your paper:
\documentclass[draft]{agujournal2019}
\usepackage{url}
\usepackage{lineno}
\usepackage[inline]{trackchanges}
\usepackage{soul}
\linenumbers

\draftfalse

\journalname{Water Resources Research}

\begin{document}

\title{Site-Aware Residual Corrections to National Water Model Streamflow Using Hybrid Transformers and Recurrent Baselines}

\authors{[Mitchel Carson]\affil{1}, [Author Two]\affil{1,2}, and [Author Three]\affil{2}}

\affiliation{1}{[Computer Science], [Appalachian State University], [Boone], [North Carolina], [United States of America]}
\affiliation{2}{[Institute or Lab], [City], [State], [Country]}

\correspondingauthor{[Corresponding Author Name]}{[email@address.edu]}

\begin{keypoints}
\item We curate a leakage-safe, hourly dataset (USGS, NWM, ERA5, NLCD) spanning 2010--2022 to enable residual post-processing of National Water Model streamflow guidance.
\item A streamlined GRU--transformer residual model attains a 20.6\% RMSE reduction on the 2022 test year, while a quantile-trimmed bias shift trades to a 13.4\% gain (NSE~0.64, KGE~0.72) and tightens percent bias to \(-2.5\%\).
\item Reproducible pipelines, quantile calibration diagnostics, and archived hyperparameter sweeps position the framework for site-to-site scaling toward \(>25\%\) improvements.
\end{keypoints}

\begin{abstract}
Operational streamflow forecasts from the National Water Model (NWM) are indispensable but retain site- and regime-dependent errors. We construct a leakage-safe, hourly dataset (2010--2022) that collocates USGS discharge (ground truth), NWM CHRTOUT retrospective and operational analyses (baseline guidance), ERA5 atmospheric reanalysis (forcings), and NLCD 2021 static metrics (physiography). Building on this dataset, we benchmark a minimalist GRU--transformer residual model that predicts \(y_{\text{USGS}} - y_{\text{NWM}}\) and reconstructs corrected flows as \(y_{\text{NWM}} + \hat{r}\). The network is trained with heteroscedastic likelihoods, an NSE-based surrogate loss, and median-focused quantile penalties to support distributional calibration. Evaluated on the held-out 2022 window, the model reduces RMSE from 5.97 to 4.74~cms (20.6\% improvement) while retaining \(-9.1\%\) bias. A quantile-trimmed residual bias adjustment yields a 5.17~cms RMSE (13.4\% improvement) with percent bias improved to \(-2.5\%\) and maintains NSE~0.64 and KGE~0.72. We document the leakage-safe pipeline, calibration diagnostics, and remaining bias challenges, outlining next steps toward \(>25\%\) improvement across diverse basins. All experiments, configurations, and artefacts are archived in the accompanying open-source repository (Data Availability Statement).
\end{abstract}

\section*{Plain Language Summary}
Forecasts from the U.S. National Water Model are widely used for rivers and streams but can remain biased at particular locations. We assembled an hourly dataset for the Watauga River in North Carolina that blends the model’s predictions with on-site USGS measurements, ERA5 weather information, and local land-cover descriptors. Machine-learning models learn the difference between the model and reality (the “residual”) and add that correction back to the forecast. A transformer-based model produced the most consistent improvement during the winter flood season, whereas a simpler recurrent baseline lagged behind. We explain how we carefully aligned the data to avoid any look-ahead leakage and outline next steps to make the corrections more reliable through the full hydrologic year.

\section{Introduction}
Physics-based hydrologic models such as the National Water Model (NWM) provide continental-scale streamflow guidance yet retain site-specific and regime-dependent errors. Data-driven post-processing that corrects model residuals can offer operational gains if it preserves temporal causality, avoids information leakage, and scales across basins. This study develops a leakage-safe hourly dataset and evaluates modern sequence models for site-aware residual correction, with an emphasis on reproducibility and transparent comparisons across architectures.

Our working hypothesis is that a rich suite of meteorological covariates—particularly those available from ERA5 and ERA5-Land—encodes much of the physics that drive systematic NWM errors. By explicitly supplying local precipitation, radiation, thermodynamic, and wind signals alongside land-cover descriptors, we expect a learning system to anticipate when NWM will under- or over-predict discharge and to correct those deviations in a causal manner. Although the present work focuses on a single, well-instrumented watershed (the Watauga River basin in North Carolina), the broader goal is to build a template that can be retrained site-by-site and eventually scaled across diverse U.S. biomes. We therefore document each pipeline step with portability in mind so that, once the residual learner is mature, it can be deployed wherever ERA5 and basic physiographic data are available. To further reduce temporal overfitting, we evaluate models with rolling-origin time-series cross-validation, an approach shown to produce unbiased risk estimates for dependent data \\citep{bergmeir2012,hyndman2021}. All code, configuration files, and experiment artefacts are archived to support independent replication (see Data Availability Statement).

\section{Methods}
We designed the study to couple leakage-safe data preparation with a residual learning model that can operate alongside the National Water Model. The workflow spans four elements: (i) curating hourly hydrometeorological records for the Watauga River gauge, (ii) enforcing causal preprocessing and feature engineering, (iii) training a transformer-based residual learner that augments a recurrent backbone, and (iv) evaluating deterministic and probabilistic skill under both single-season and rolling cross-validation regimes.

\paragraph{Diagram placeholder.} Future Figure~1 will summarise the end-to-end pipeline from raw data ingestion to evaluation outputs.

\subsection{Data Sources and Acquisition}
Data acquisition follows the audited scripts that ship with the code base. Hourly USGS discharge from the NWIS API (stored under \texttt{data/raw/usgs/}) provides the reference truth. Operational and retrospective National Water Model CHRTOUT products are retrieved through authenticated S3 requests; for 2010--2020 we use the v2.1 retrospective archive, while 2021--2022 leverage the v3.0 retrospective bucket to ensure complete hourly coverage. ERA5-Land reanalysis tiles supply meteorological covariates---precipitation, temperature, humidity, pressure, radiation, and wind components---downscaled to the gauge coordinates. Static descriptors originate from NLCD 2021 fractional land-cover metrics and regulation status flags that capture the watershed’s forested headwaters and limited urban footprint. Each download is logged with command strings, hashes, and extraction dates so the collection can be replayed.

\paragraph{Figure placeholder.} Future Figure~2 will map the study area, overlaying the Watauga gauge and summarising dominant land-cover fractions.

\subsection{Preprocessing and Dataset Assembly}
All sources are aligned to the hourly USGS timeline using left-aligned joins to prevent look-ahead bias. Missing dynamic values trigger gap logging rather than silent interpolation; static fields are forward-filled only when the upstream dataset prescribes a single categorical value. Feature engineering introduces trigonometric encodings for sub-daily and seasonal cycles, antecedent precipitation indices, and flow-normalised residual targets \(y_{\text{residual}} = y_{\text{USGS}} - y_{\text{NWM}}\). Normalisation statistics are computed exclusively on the 2010--2020 training period and cached to disk, so validation (2021) and test (2022) windows reuse identical scalers. The aligned frame is wrapped in a \texttt{SeqDataset} object that emits rolling windows with stride one, returning dynamic tensors, static vectors, residual targets, and baseline flows for every timestamp. Deterministic splits (train: 2010--2020, validation: 2021, test: 2022) are enforced per site, while an auxiliary rolling-origin cross-validation driver generates nine folds whose training spans end in 2012--2020 with the next year held out, mimicking deployment risk.

\paragraph{Figure placeholder.} Future Figure~3 will illustrate the temporal splits and sliding-window sampling strategy.

\subsection{Modeling Strategy and Transformer Rationale}
The residual learner combines a compact gated recurrent unit (GRU) encoder with a two-layer transformer encoder (\(d_{\text{model}} = 64\), four heads). We benchmarked pure GRU, LSTM, and temporal convolutional variants early in the study, but they struggled to maintain skill once we expanded the forcing set and enforced causal validation. The transformer block improves generalisation by letting the model reweight multi-day meteorological patterns without deep recurrent stacks, while the GRU handles short-term persistence at low computational cost. Cross-attention with static embeddings allows watershed attributes (e.g., forest cover) to modulate the dynamic response, an ability we found lacking in the purely recurrent baselines. We retain a residual-first formulation---predicting \(\hat{r}\) and reconstructing discharge as \(y_{\text{NWM}} + \hat{r}\)---to stay consistent with physics-based priors and to guarantee that the corrected series remains anchored to the NWM hydrograph. Although physics-informed surrogates such as advanced rainfall-runoff models were considered, their calibration overhead and lack of native uncertainty estimates made them ill-suited for the rapid sweep cycles required here; the transformer hybrid delivered higher NSE and KGE in validation while remaining lightweight enough for hourly updates.

Multi-scale pooling condenses the transformer output: we concatenate the final hidden state, global mean and maximum, an attention-derived context vector, and outputs from dilated 1D convolutions (dilation 1, 3, 6). When static data are available, a learned query attends over the dynamic sequence to produce a static-aware context vector that is fused with the dynamic summaries via a shallow multilayer perceptron. Output heads emit (i) the residual mean \(\hat{r}\) with a learned scalar bias, (ii) residual log-variance, (iii) corrected-flow log-variance, and (iv) optional quantiles for uncertainty bands. This structure proved more sample-efficient than deeper attention stacks and allowed us to reuse checkpoints when moving between bias-penalised objectives.

\paragraph{Figure placeholder.} Future Figure~4 will diagram the hybrid GRU--transformer architecture and feature fusion pathway.

\subsection{Training and Evaluation Protocol}
Training uses AdamW with gradient clipping (1.0) and cosine decay on the learning rate unless a plateau scheduler triggers early stabilisation. Mini-batches preserve temporal order to respect autodependence. The composite loss blends heteroscedastic Gaussian negative log-likelihood terms on both residual and reconstructed flows, an NSE-inspired differentiable surrogate, a median-focused pinball loss to stabilise the 50th-percentile forecast, and a mild squared bias penalty. Hyperparameter sweeps explored learning rates from \(2\times10^{-4}\) to \(1\times10^{-3}\), batch sizes from 64 to 1024, and bias-control weights; the best-performing configuration (batch 1024, \(8\times10^{-4}\), shift strength 0.1) is archived as \texttt{archive/baselines/batchsweep\_bs1024\_lr8e4\_shift01\_20251020.json}. Early stopping monitors validation loss with patience of five epochs, and checkpoints store optimizer state to resume long sweeps.

Evaluation covers the 2022 deployment year and the nine-fold rolling validation schedule. We report RMSE, MAE, NSE, Kling-Gupta efficiency, percent bias, and rank correlations, alongside probabilistic coverage for the optional quantile heads. All predictions, calibration diagnostics, and configuration manifests are versioned so that figures and tables in the manuscript can be regenerated. Future Figure~5 will synthesise the evaluation pipeline and cross-validation folds.

\section{Results (Watauga Basin)}
The archived best configuration (\texttt{batchsweep\_bs1024\_lr8e4\_shift01}) surpasses the raw NWM guidance on every headline metric for the 2022 deployment year. Relative to the NWM baseline (RMSE 5.97~cms, NSE 0.52, KGE 0.64, PBIAS \(-6.8\%\)), the residual model attains an RMSE of 4.92~cms (17.6\% reduction), NSE of 0.673, KGE of 0.724, and narrows percent bias to \(-1.73\%\). The improvements concentrate during cold-season high-flow episodes where the transformer can leverage multi-day precipitation signatures; nevertheless, low-flow periods remain well behaved because the residual formulation keeps the corrected hydrograph anchored to NWM discharge.

We evaluated a lighter post-processing adjustment—a quantile-trimmed residual shift computed over the 5th to 90th percentile validation flows—to trade a small amount of RMSE for additional bias control. Applying the shift increases RMSE modestly to 5.17~cms (13.4\% reduction versus NWM) while tightening PBIAS to \(-2.5\%\) and maintaining NSE \(\approx 0.64\) and KGE \(\approx 0.72\). The shift also raises median coverage above 60\%, though lower quantiles remain under-dispersed, indicating that further calibration is needed for the probabilistic heads.

Rolling-origin cross-validation confirms the deployment-year gains and reveals where the model underperforms. Across nine folds (2013--2021 validation years) the transformer achieves mean RMSE improvements of 7.7\% and a mean NSE increase of 0.06 over NWM, yet KGE slips slightly and late-cycle folds (2019--2021) retain percent biases near \(-6\%\). These diagnostics motivate the adaptive bias-control experiments discussed in the companion sweeps and highlight the value of year-by-year scrutiny before scaling to additional gauges.

\paragraph{Figure placeholder.} Future Figure~6 will present observed, NWM, and corrected hydrographs with uncertainty envelopes, while accompanying panels will summarise monthly skill, residual distributions, and fold-level metrics for the Watauga site.

\section{Discussion and Future Work}
The Watauga case study shows that a hybrid GRU--transformer residual learner can deliver simultaneous improvements in RMSE, NSE, KGE, and percent bias relative to the operational NWM forecast, validating our decision to prioritise attention mechanisms over purely recurrent or physics-calibrated alternatives. The model captures multi-day storm signatures without sacrificing low-flow fidelity, and the residual formulation keeps corrections physically grounded. Importantly, the rolling-origin analysis reveals that bias control erodes in recent years, so adaptive penalties or validation-aware shifts remain essential before expanding to additional basins.

Our experiments indicate that transformer attention offers two practical advantages over LSTM-heavy architectures tested earlier: parameter efficiency (fewer layers required to match skill during sweeps) and an interpretable pathway for incorporating static descriptors. Physics-informed surrogates remain valuable for process insight, yet their calibration cost and limited uncertainty quantification made them less compatible with the rapid iterate-and-evaluate loop demanded by the thesis timeline. The present approach therefore complements the NWM rather than replacing it, acting as a data-driven correction layer that can be tuned as new forcing generations arrive.

Ongoing work will (i) broaden the hyperparameter search over consistency, NSE, and bias weights now that the base model is stable; (ii) explore richer static--dynamic cross-attention mechanisms; (iii) integrate bias-aware calibration objectives directly into training to reduce reliance on post-hoc adjustments; (iv) package the end-to-end pipeline into reproducible releases; and (v) extend evaluation to additional basins (e.g., Au Sable River, MI) to test portability. Achieving \(\geq 25\%\) improvement will likely require both architectural refinement and informed loss shaping, but the current multi-metric gain provides a concrete starting point for operational integration.

\subsection{Downstream Decision Support Opportunities}
The National Water Model feeds the National Weather Service’s National Water Prediction Service (NWPS), where hourly to multi-day discharge forecasts populate map-based products, AWIPS decision dashboards, and River Forecast Center workflows. Operational hydrologists blend NWM guidance with local hydraulic models and expert judgment to issue flood watches, reservoir operations advisories, and navigation bulletins; partner agencies such as USACE, USGS, and emergency management offices consume the same feeds for situational awareness and resource allocation. Because these services depend on rapid access to bias-aware streamflow predictions, a transformer-based post-processor can slot in as a lightweight corrective layer: corrected flows can be published alongside raw NWM guidance, enabling forecasters to see both the baseline and the bias-adjusted ensemble, or ingested into automated alert thresholds where reduced percent-bias and improved NSE/KGE translate directly into fewer false alarms and missed events. Beyond deterministic guidance, the transformer’s quantile head offers calibrated uncertainty summaries that could support probabilistic flood triggers once post-hoc calibration is complete. Integrating the post-processor therefore promises not only improved accuracy metrics but also more trustworthy decision support across flood warning, reservoir scheduling, and impact-based communication.

\begin{thebibliography}{99}
ibitem[\textit{Bergmeir and Ben\'{i}tez}(2012)]{bergmeir2012} Bergmeir, C., and J. M. Ben\'{i}tez (2012), On the use of cross-validation for time series predictor evaluation, \textit{Information Sciences}, \textit{191}, 192--213, doi:10.1016/j.ins.2011.12.028.

ibitem[\textit{Hyndman and Athanasopoulos}(2021)]{hyndman2021} Hyndman, R. J., and G. Athanasopoulos (2021), \textit{Forecasting: Principles and Practice}, 3rd ed., OTexts, Melbourne, Australia. Retrieved from https://otexts.com/fpp3/.
\end{thebibliography}

\end{document}
