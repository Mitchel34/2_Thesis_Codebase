%% To submit your paper:
\documentclass[draft]{agujournal2019}
\usepackage{url}
\usepackage{lineno}
\usepackage[inline]{trackchanges}
\usepackage{soul}
\linenumbers

\draftfalse

\journalname{Water Resources Research}

\begin{document}

\title{Site-Aware Residual Corrections to National Water Model Streamflow Using Hybrid Transformers and Recurrent Baselines}

\authors{[Mitchel Carson]\affil{1}, [Author Two]\affil{1,2}, and [Author Three]\affil{2}}

\affiliation{1}{[Computer Science], [Appalachian State University], [Boone], [North Carolina], [United States of America]}
\affiliation{2}{[Institute or Lab], [City], [State], [Country]}

\correspondingauthor{[Corresponding Author Name]}{[email@address.edu]}

\begin{keypoints}
\item We curate a leakage-safe, hourly dataset (USGS, NWM, ERA5, NLCD) spanning 2010--2022 to enable residual post-processing of National Water Model streamflow guidance.
\item A streamlined GRU--transformer residual model attains a 20.6\% RMSE reduction on the 2022 test year, while a quantile-trimmed bias shift trades to a 13.4\% gain (NSE~0.64, KGE~0.72) and tightens percent bias to \(-2.5\%\).
\item Reproducible pipelines, quantile calibration diagnostics, and archived hyperparameter sweeps position the framework for site-to-site scaling toward \(>25\%\) improvements.
\end{keypoints}

\begin{abstract}
Operational streamflow forecasts from the National Water Model (NWM) are indispensable but retain site- and regime-dependent errors. We construct a leakage-safe, hourly dataset (2010--2022) that collocates USGS discharge (ground truth), NWM CHRTOUT retrospective and operational analyses (baseline guidance), ERA5 atmospheric reanalysis (forcings), and NLCD 2021 static metrics (physiography). Building on this dataset, we benchmark a minimalist GRU--transformer residual model that predicts \(y_{\text{USGS}} - y_{\text{NWM}}\) and reconstructs corrected flows as \(y_{\text{NWM}} + \hat{r}\). The network is trained with heteroscedastic likelihoods, an NSE-based surrogate loss, and median-focused quantile penalties to support distributional calibration. Evaluated on the held-out 2022 window, the model reduces RMSE from 5.97 to 4.74~cms (20.6\% improvement) while retaining \(-9.1\%\) bias. A quantile-trimmed residual bias adjustment yields a 5.17~cms RMSE (13.4\% improvement) with percent bias improved to \(-2.5\%\) and maintains NSE~0.64 and KGE~0.72. We document the leakage-safe pipeline, calibration diagnostics, and remaining bias challenges, outlining next steps toward \(>25\%\) improvement across diverse basins. All experiments, configurations, and artefacts are archived in the accompanying open-source repository (Data Availability Statement).
\end{abstract}

\section*{Plain Language Summary}
Forecasts from the U.S. National Water Model are widely used for rivers and streams but can remain biased at particular locations. We assembled an hourly dataset for the Watauga River in North Carolina that blends the model’s predictions with on-site USGS measurements, ERA5 weather information, and local land-cover descriptors. Machine-learning models learn the difference between the model and reality (the “residual”) and add that correction back to the forecast. A transformer-based model produced the most consistent improvement during the winter flood season, whereas a simpler recurrent baseline lagged behind. We explain how we carefully aligned the data to avoid any look-ahead leakage and outline next steps to make the corrections more reliable through the full hydrologic year.

\section{Introduction}
Physics-based hydrologic models such as the National Water Model (NWM) provide continental-scale streamflow guidance yet retain site-specific and regime-dependent errors. Data-driven post-processing that corrects model residuals can offer operational gains if it preserves temporal causality, avoids information leakage, and scales across basins. This study develops a leakage-safe hourly dataset and evaluates modern sequence models for site-aware residual correction, with an emphasis on reproducibility and transparent comparisons across architectures.

Our working hypothesis is that a rich suite of meteorological covariates—particularly those available from ERA5 and ERA5-Land—encodes much of the physics that drive systematic NWM errors. By explicitly supplying local precipitation, radiation, thermodynamic, and wind signals alongside land-cover descriptors, we expect a learning system to anticipate when NWM will under- or over-predict discharge and to correct those deviations in a causal manner. Although the present work focuses on a single, well-instrumented watershed (the Watauga River basin in North Carolina), the broader goal is to build a template that can be retrained site-by-site and eventually scaled across diverse U.S. biomes. We therefore document each pipeline step with portability in mind so that, once the residual learner is mature, it can be deployed wherever ERA5 and basic physiographic data are available. To further reduce temporal overfitting, we evaluate models with rolling-origin time-series cross-validation, an approach shown to produce unbiased risk estimates for dependent data \\citep{bergmeir2012,hyndman2021}. All code, configuration files, and experiment artefacts are archived to support independent replication (see Data Availability Statement).

\section{Methods}
We designed the study to couple leakage-safe data preparation with a residual learning model that operates alongside the National Water Model. The workflow spans data curation, causal feature engineering, and the training of a transformer-based residual learner.

\subsection{Data Curation and Preprocessing}
Data acquisition follows the audited scripts that ship with the code base. Hourly USGS discharge from the NWIS API (stored under \texttt{data/raw/usgs/}) provides the reference truth. Operational and retrospective National Water Model CHRTOUT products are retrieved through authenticated S3 requests; for 2010--2020 we use the v2.1 retrospective archive, while 2021--2022 leverage the v3.0 retrospective bucket to ensure complete hourly coverage. ERA5-Land reanalysis tiles supply meteorological covariates---precipitation, temperature, humidity, pressure, radiation, and wind components---downscaled to the gauge coordinates. Static descriptors originate from NLCD 2021 fractional land-cover metrics and regulation status flags that capture the watershed’s forested headwaters and limited urban footprint. Each download is logged with command strings, hashes, and extraction dates so the collection can be replayed.

All sources are aligned to the hourly USGS timeline using left-aligned joins to prevent look-ahead bias. Missing dynamic values trigger gap logging rather than silent interpolation; static fields are forward-filled only when the upstream dataset prescribes a single categorical value. Feature engineering introduces trigonometric encodings for sub-daily and seasonal cycles, antecedent precipitation indices, and flow-normalised residual targets \(y_{\text{residual}} = y_{\text{USGS}} - y_{\text{NWM}}\). Normalisation statistics are computed exclusively on the 2010--2020 training period and cached to disk, so validation (2021) and test (2022) windows reuse identical scalers. The aligned frame is wrapped in a \texttt{SeqDataset} object that emits rolling windows with stride one, returning dynamic tensors, static vectors, residual targets, and baseline flows for every timestamp. Deterministic splits (train: 2010--2020, validation: 2021, test: 2022) are enforced per site, while an auxiliary rolling-origin cross-validation driver generates nine folds whose training spans end in 2012--2020 with the next year held out, mimicking deployment risk.

\subsection{Hybrid Residual Learning Framework}
The residual learner combines a compact gated recurrent unit (GRU) encoder with a two-layer transformer encoder (\(d_{\text{model}} = 64\), four heads). We benchmarked pure GRU, LSTM, and temporal convolutional variants early in the study, but they struggled to maintain skill once we expanded the forcing set and enforced causal validation. The transformer block improves generalisation by letting the model reweight multi-day meteorological patterns without deep recurrent stacks, while the GRU handles short-term persistence at low computational cost. Cross-attention with static embeddings allows watershed attributes (e.g., forest cover) to modulate the dynamic response, an ability we found lacking in the purely recurrent baselines. We retain a residual-first formulation---predicting \(\hat{r}\) and reconstructing discharge as \(y_{\text{NWM}} + \hat{r}\)---to stay consistent with physics-based priors and to guarantee that the corrected series remains anchored to the NWM hydrograph.

Multi-scale pooling condenses the transformer output: we concatenate the final hidden state, global mean and maximum, an attention-derived context vector, and outputs from dilated 1D convolutions (dilation 1, 3, 6). When static data are available, a learned query attends over the dynamic sequence to produce a static-aware context vector that is fused with the dynamic summaries via a shallow multilayer perceptron. Output heads emit (i) the residual mean \(\hat{r}\) with a learned scalar bias, (ii) residual log-variance, (iii) corrected-flow log-variance, and (iv) optional quantiles for uncertainty bands. This structure proved more sample-efficient than deeper attention stacks and allowed us to reuse checkpoints when moving between bias-penalised objectives.

Training uses AdamW with gradient clipping (1.0) and cosine decay on the learning rate unless a plateau scheduler triggers early stabilisation. Mini-batches preserve temporal order to respect autodependence. The composite loss blends heteroscedastic Gaussian negative log-likelihood terms on both residual and reconstructed flows, an NSE-inspired differentiable surrogate, a median-focused pinball loss to stabilise the 50th-percentile forecast, and a mild squared bias penalty. Hyperparameter sweeps explored learning rates from \(2\times10^{-4}\) to \(1\times10^{-3}\), batch sizes from 64 to 1024, and bias-control weights; the best-performing configuration (batch 1024, \(8\times10^{-4}\), shift strength 0.1) is archived as \texttt{archive/baselines/batchsweep\_bs1024\_lr8e4\_shift01\_20251020.json}. Early stopping monitors validation loss with patience of five epochs, and checkpoints store optimizer state to resume long sweeps. Evaluation covers the 2022 deployment year and the nine-fold rolling validation schedule. We report RMSE, MAE, NSE, Kling-Gupta efficiency, percent bias, and rank correlations, alongside probabilistic coverage for the optional quantile heads.

\section{Results (Watauga Basin)}
The archived best configuration (\texttt{batchsweep\_bs1024\_lr8e4\_shift01}) surpasses the raw NWM guidance on every headline metric for the 2022 deployment year. Relative to the NWM baseline (RMSE 5.97~cms, NSE 0.52, KGE 0.64, PBIAS \(-6.8\%\)), the residual model attains an RMSE of 4.92~cms (17.6\% reduction), NSE of 0.673, KGE of 0.724, and narrows percent bias to \(-1.73\%\). The improvements concentrate during cold-season high-flow episodes where the transformer can leverage multi-day precipitation signatures; nevertheless, low-flow periods remain well behaved because the residual formulation keeps the corrected hydrograph anchored to NWM discharge.

We evaluated a lighter post-processing adjustment—a quantile-trimmed residual shift computed over the 5th to 90th percentile validation flows—to trade a small amount of RMSE for additional bias control. Applying the shift increases RMSE modestly to 5.17~cms (13.4\% reduction versus NWM) while tightening PBIAS to \(-2.5\%\) and maintaining NSE \(\approx 0.64\) and KGE \(\approx 0.72\). The shift also raises median coverage above 60\%, though lower quantiles remain under-dispersed, indicating that further calibration is needed for the probabilistic heads.

Rolling-origin cross-validation confirms the deployment-year gains and reveals where the model underperforms. Across nine folds (2013--2021 validation years) the transformer achieves mean RMSE improvements of 7.7\% and a mean NSE increase of 0.06 over NWM, yet KGE slips slightly and late-cycle folds (2019--2021) retain percent biases near \(-6\%\). These diagnostics motivate the adaptive bias-control experiments discussed in the companion sweeps and highlight the value of year-by-year scrutiny before scaling to additional gauges.

\paragraph{Figure placeholder.} Future Figure~6 will present observed, NWM, and corrected hydrographs with uncertainty envelopes, while accompanying panels will summarise monthly skill, residual distributions, and fold-level metrics for the Watauga site.

\section{Discussion}
The Watauga case study demonstrates that a hybrid GRU--transformer residual learner delivers simultaneous improvements in RMSE, NSE, KGE, and percent bias relative to the operational NWM forecast. The model captures multi-day storm signatures without sacrificing low-flow fidelity, and the residual formulation keeps corrections physically grounded. However, the performance is not uniform across all regimes. While the 2022 test year showed a robust 20.6\% RMSE reduction, analysis of the January 2023 period reveals a degradation in skill, with the corrected RMSE (1.68~cms) exceeding the NWM baseline (1.44~cms). This winter instability correlates with the basin's physiography; the Watauga watershed is 62.43\% forested and unregulated, suggesting that complex snow-canopy interactions or rain-on-snow events may drive errors that the current ERA5-based feature set does not fully resolve. The positive bias observed during these winter months indicates the model may overcorrect when anticipating precipitation responses that are actually dampened by snowpack storage.

These findings highlight the trade-off between aggregate annual improvement and seasonal reliability. The transformer attention mechanism successfully reweights meteorological inputs to correct systematic NWM errors during standard flow regimes, but it lacks the explicit process constraints to handle phase transitions like snowmelt with high precision. Future iterations must address this by incorporating snow water equivalent (SWE) state variables or implementing regime-specific loss functions that penalise winter over-prediction more heavily. Additionally, the rolling-origin analysis confirms that bias control erodes in recent years, reinforcing the need for adaptive penalties or validation-aware shifts before scaling to additional basins.

Despite these seasonal limitations, the framework offers immediate utility for operational decision support. The National Water Model feeds the National Weather Service’s National Water Prediction Service (NWPS), where hourly to multi-day discharge forecasts populate map-based products and decision dashboards. Operational hydrologists blend NWM guidance with local hydraulic models and expert judgment to issue flood watches and reservoir operations advisories. A transformer-based post-processor can slot in as a lightweight corrective layer: corrected flows can be published alongside raw NWM guidance, enabling forecasters to see both the baseline and the bias-adjusted ensemble. Integrating the post-processor promises improved accuracy metrics and more trustworthy decision support across flood warning and reservoir scheduling, provided that forecasters are presented with the uncertainty bounds that capture the model's lower confidence during winter instability.

\section{Conclusion}
We presented a reproducible, leakage-safe framework for correcting National Water Model streamflow residuals using a hybrid GRU--transformer architecture. By curating a high-fidelity hourly dataset for the Watauga River basin and enforcing strict causal validation, we demonstrated that data-driven post-processing can reduce RMSE by over 20\% and significantly improve NSE and KGE relative to the raw physics-based guidance. The study establishes that meteorological covariates from ERA5, when processed through attention mechanisms, contain sufficient signal to correct systematic hydrologic errors. However, the analysis also uncovered critical seasonal limitations, specifically winter performance degradations in heavily forested, snow-influenced catchments. These insights define a clear path for future research: integrating snow-specific state variables, refining loss functions for seasonal robustness, and expanding the evaluation to diverse hydroclimatic regions. Ultimately, this work provides a validated template for site-aware operational corrections, moving toward a scalable system that enhances the reliability of continental-scale water prediction.

\begin{thebibliography}{99}
\bibitem[\textit{Bergmeir and Ben\'{i}tez}(2012)]{bergmeir2012} Bergmeir, C., and J. M. Ben\'{i}tez (2012), On the use of cross-validation for time series predictor evaluation, \textit{Information Sciences}, \textit{191}, 192--213, doi:10.1016/j.ins.2011.12.028.

\bibitem[\textit{Hyndman and Athanasopoulos}(2021)]{hyndman2021} Hyndman, R. J., and G. Athanasopoulos (2021), \textit{Forecasting: Principles and Practice}, 3rd ed., OTexts, Melbourne, Australia. Retrieved from https://otexts.com/fpp3/.
\end{thebibliography}

\end{document}

\begin{thebibliography}{99}

\subsection{Downstream Decision Support Opportunities}
The National Water Model feeds the National Weather Service’s National Water Prediction Service (NWPS), where hourly to multi-day discharge forecasts populate map-based products, AWIPS decision dashboards, and River Forecast Center workflows. Operational hydrologists blend NWM guidance with local hydraulic models and expert judgment to issue flood watches, reservoir operations advisories, and navigation bulletins; partner agencies such as USACE, USGS, and emergency management offices consume the same feeds for situational awareness and resource allocation. Because these services depend on rapid access to bias-aware streamflow predictions, a transformer-based post-processor can slot in as a lightweight corrective layer: corrected flows can be published alongside raw NWM guidance, enabling forecasters to see both the baseline and the bias-adjusted ensemble, or ingested into automated alert thresholds where reduced percent-bias and improved NSE/KGE translate directly into fewer false alarms and missed events. Beyond deterministic guidance, the transformer’s quantile head offers calibrated uncertainty summaries that could support probabilistic flood triggers once post-hoc calibration is complete. Integrating the post-processor therefore promises not only improved accuracy metrics but also more trustworthy decision support across flood warning, reservoir scheduling, and impact-based communication.

\begin{thebibliography}{99}
ibitem[\textit{Bergmeir and Ben\'{i}tez}(2012)]{bergmeir2012} Bergmeir, C., and J. M. Ben\'{i}tez (2012), On the use of cross-validation for time series predictor evaluation, \textit{Information Sciences}, \textit{191}, 192--213, doi:10.1016/j.ins.2011.12.028.

ibitem[\textit{Hyndman and Athanasopoulos}(2021)]{hyndman2021} Hyndman, R. J., and G. Athanasopoulos (2021), \textit{Forecasting: Principles and Practice}, 3rd ed., OTexts, Melbourne, Australia. Retrieved from https://otexts.com/fpp3/.
\end{thebibliography}

\end{document}
